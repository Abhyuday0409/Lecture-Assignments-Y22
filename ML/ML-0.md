# ML-0 Assignment

Made by: Tushar Sahu and Aryan Khanuja

## Question 1
**Explain** the following question answer in detail:
> Q: You are doing full batch gradient descent using the entire training set (not stochastic gradient descent), Is it necessary to shuffle the training data? Explain your answer.
>
> A: It is not necessary. Each iteration of full batch gradient descent runs through the entire dataset and therefore order of dataset does not matter.

## Question 2
Implement Gradient Descent in a Jupyter Notebook. Also explain in the same file how tuning each hyperparameter changes the outcome?

## Question 3
Go through first 2 chapters of ISLR. Now, explain Bias-Variance tradeoff in detail.

## How to Submit?
Submit a well-formatted Jupyter Notebook. Use Markdown cells to separate each question and for any explanation that you wish to provide.

Create your notebook inside `ML-0-Submissions/` and name it as `<your>-<name>.ipynb`
