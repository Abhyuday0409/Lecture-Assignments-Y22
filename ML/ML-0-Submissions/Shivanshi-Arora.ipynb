{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matpotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0681632ec0ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatpotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matpotlib'"
     ]
    }
   ],
   "source": [
    "#Q1 Ans:\n",
    "#In full batch gradient descent, shuffling of data is not necessary. Since, in batch gradient descent, we work upon the whole set of data once, therefore it doesnt matter which data entries appear first and which later on, i.e, order does not matter.\n",
    "\n",
    "#Q2 Ans:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "x = np.array([77,21,22,20,36,15,62,95,20,5,4,19,96,62,36,15,65])\n",
    "y = np.array([79.77515201,23.17727887,25.60926156,17.85738813,41.84986439,9.805234876,58.87465933,97.61793701,18.39512747,8.746747654,2.811415826,17.09537241,95.14907176,61.38800663,40.24701716,14.82248589,66.95806869])\n",
    "pyplot.scatter(x,y)\n",
    "pyplot.show\n",
    "\n",
    "m = 0.01\n",
    "c = 0.01\n",
    "l = 0.0002\n",
    "epoch = 500\n",
    "\n",
    "n=len(y)\n",
    "\n",
    "for i in range(epoch):\n",
    "    y0 = (m*x)+c\n",
    "    m_der = (-2/n)*np.dot((y-y0),x)\n",
    "    c_der = (-2/n)*sum(y-y0)\n",
    "    m = m-(l*m_der)\n",
    "    c = c-(l*c_der)\n",
    "    cost_func= (1/n)*sum((y0-y)**2)\n",
    "    \n",
    "y0 = m*x + c    \n",
    "pyplot.scatter(x,y)\n",
    "pyplot.plot(x,m*x +c, color='blue')\n",
    "pyplot.show\n",
    "\n",
    "print(\"y=\", m,\"*x + \" ,c)\n",
    "\n",
    "#Tuning the hyperparameters,which are learning rate and epochs in this case, change the outcomes. In case of a larger learning rate, the error in gradient descent would become considerable rather than being minimised. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
